{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UBVOWl7-Fgq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st iteration(don't delete cells)\n",
        "df=pd.read_csv('/content/flipkart_com-ecommerce_sample.csv')\n",
        "\n",
        "##### RECOMMENDATION PROCESS STARTS\n",
        "\n",
        "## TF-IDF starts\n",
        "\n",
        "max_levels = 0\n",
        "for item in df['product_category_tree']:\n",
        "    count = len(item.split(\">>\"))\n",
        "    if count > max_levels:\n",
        "        max_levels = count\n",
        "\n",
        "# print(\"The data has\", max_levels, \"levels\")\n",
        "\n",
        "\n",
        "# create an empty list to store the split categories\n",
        "split_category_list = []\n",
        "\n",
        "for category in df['product_category_tree']:\n",
        "    # spliting\n",
        "    split_category = category.split('>>')\n",
        "    # spliting and putting in 2D list\n",
        "    split_category_list.append(split_category)\n",
        "\n",
        "# print(split_category_list[1:10])\n",
        "\n",
        "# breaked into individual words\n",
        "words = []\n",
        "for categories in split_category_list:\n",
        "    for category in categories:\n",
        "        words += category.split()\n",
        "\n",
        "# print(words[1:30])\n",
        "words = [word for word in words if len(word) > 1]\n",
        "\n",
        "words = [w.replace(\"'\", \"\").replace('\"', '').replace('(', '').\n",
        "             replace(')', '').replace('[', '').\n",
        "         replace(']', '').replace('...', '').\n",
        "         replace(',', '')for w in words]\n",
        "\n",
        "# print('Words\\n',words[1:30])\n",
        "\n",
        "#unique_words = []\n",
        "# for word in words:\n",
        "#     if word not in unique_words: # jab pehly sy hi hoga tu  dobara nahi jaey ga tokri my\n",
        "#         unique_words.append(word)\n",
        "\n",
        "\n",
        "unique_words = set(words) # removes duplicates\n",
        "unique_words=sorted(list(unique_words))\n",
        "\n",
        "# Define the list of words to remove #remove strat\n",
        "words_to_remove = ['', '&', '+1', '-', '-446', '-522', '-583', '-B150AC', '-R-MEDIUM3', '-Therap', '-value', '.43X', '0', '0-100', '0-15', '0.5', '0019', '002-SE', '0024', '011', '01400-0015', '017', '03', '0303', '04', '0401-002', '05', '05102', '052', '06', '0622', '07', '08HD-ES-62', '1', '1-g--nike-m', '1-st1b--nike-m', '1.5', '1.75', '10', '10%', '10.5v2.9a', '100', '100%', '100/90-17', '1000', '10000', '10000mAh', '10000mah', '1000m', '1005', '1019', '1033', '1038', '1042', '105key', '106', '1066.', '107751', '109F', '109f', '10X70X70', '10\\\\', '11', '110', '112', '1150', '119M', '12', '12.7', '120', '1200', '12000', '1219', '122/1000M', '123', '125g', '12V/24V', '12W', '12\\\\', '13', '13.97', '1320', '135ml', '138246', '14', '14.5', '140/70-17', '142492', '144', '145', '145744', '147', '148', '149', '14K', '15', '15.24X20.32', '15.6', '150', '1500', '150013', '15052HTCX9-SKN', '15059HTCX9-SKN', '1514', '1524', '154', '154222', '154226', '154235', '154243', '154253', '154257', '154266', '154267', '154274', '154280', '154298', '154314', '154317', '154320', '154324', '154325', '154326', '154331', '154357', '154388', '154402', '154422', '154433', '154436', '154438', '154439', '155489', '157466', '157497', '15cmx10cm', '15mtr', '16', '16-510', '160/2gb/DDR2', '1600W', '161', '165', '166', '169', '169-CIN', '17', '170', '1706201621', '170760', '170774', '170783', '170792', '170798', '170817', '170822', '170831', '170906', '170909', '171', '172', '17500', '17502', '17505', '17507', '176', '179', '18', '18.5', '180', '1828.8', '1829', '183', '186', '186136', '186879', '186926', '187', '18K', '18W', '19', '19.5', '19.5v3.9a', '190', '1905', '194', '196', '1981.199999999', '1OAK', '1Pair', '1kg', '1m189', '1pack', '1pcs', '1pen', '1x2GB', '2', '2-D-', '2-Fo', '2-Ma', '2-O-', '2.0', '2.1', '2.25', '2.5', '2.75-17', '2.75-18', '2.8', '20', '20%', '20.2', '20.4', '200', '2000', '2006', '2015-2016', '2016-2017', '20200W', '202050', '202051', '202056', '202058', '20503', '207232', '208', '20881', '20mtr', '21', '210', '2110', '2113', '2133.6', '216', '216008', '216456', '216510', '216552', '217', '218', '2188-4', '22', '220', '220102', '220106', '220108', '220113', '220164', '220165', '220175', '220179', '225', '225-Button', '22K', '22nd', '23', '23000', '2300W', '232241', '2323', '232305', '232358', '2333', '236', '24', '240', '240306', '240318', '240319', '240322', '240325', '240327', '240342', '240350', '240362', '240363', '240371', '240403', '240404', '240408', '240411', '240420', '240421', '240428', '240437', '240439', '240440', '240450', '240459', '240460', '240461', '245', '247', '24K', '24ct', '24kt', '25', '250', '256-SPM', '25868', '26', '2615.023.132', '26cm', '27', '270', '273', '28', '280', '280r', '290', '2nd', '3', '3-speed', '3.0', '3.00-18', '3.1', '3.2', '3.4', '3.5', '3.50-10', '3.50-19', '3.5mm', '3/4', '3/4ths', '30', '30.4', '300', '3000', '31', '32', '320', '325', '3250', '33568', '33649', '34', '343', '35', '35.56', '350', '3500i', '3514', '354', '36', '360', '365', '38', '3Bright', '3D', '3D+', '3R', '3d', '3g', '3in1', '3kFactory', '3wish', '4', '4.00-8', '4.1', '4.25g', '4.26', '4.3', '4.5', '4.8V', '4/4s/4g/5/5c', '40', '400', '4000', '4102', '420', '4202', '428', '43', '434', '45', '450', '456', '45W', '46', '460', '475', '476', '477', '48', '483', '497', '4C', '4D', '4G', '4K', '4S', '4U', '4X', '4i', '4mm', '4s', '4thneed', '5', '5.5', '5.5x5.5', '50', '500', '5000', '5000mah', '502', '503TF_BLK', '51', '510', '52', '520', '526', '52Mm', '52mm', '53', '54', '55', '550', '56', '56m', '57', '58', '59', '591', '5921', '5A', '5I', '5SL', '5X', '6', '6.35', '60', '600', '60W', '61', '612', '62', '620', '633D', '64', '66', '67', '67mm', '684658-003', '69SS0002-X1', '69SS0003-B6', '69th', '6LED', '6P', '6S', '6W', '7', '7.6', '7.62', '70', '720', '7230', '728S', '73171', '75', '750', '7512_TM_P', '7562', '762', '77', '77306', '77341', '77365', '77370', '77374', '77489', '77538', '77551', '77555', '77565', '77595', '77599', '7I', '7\\\\', '7in', '8', '8.646', '80', '800', '83.820', '85', '8520', '85W', '86', '883', '8908-1', '8K', '8\\\\', '9', '9.40', '9.8250000000', '90', '91', '9107or', '9340', '9500', '99', '999', '999store', '99Gems', '99Hunts', '99Moves', '9LED', '9V', '9\\\\', '@499', '@home']\n",
        "\n",
        "# Remove the words from the list\n",
        "for word in words_to_remove:\n",
        "    while word in unique_words:\n",
        "        unique_words.remove(word)\n",
        "\n",
        "# remove words with less than 2 length\n",
        "unique_words = [word for word in unique_words if len(word) > 2] # we can remove words to remove finctionlity\n",
        "#remove end\n",
        "\n",
        "# Print the updated list\n",
        "# print('Unique words\\n', unique_words[1:30])\n",
        "\n",
        "# print('Length: ',len(unique_words))\n",
        "\n",
        "# if 'Womens' in unique_words:\n",
        "#     print('Yes, the list contains \"Womens\"')\n",
        "# else:\n",
        "#     print('No, the list does not contain \"Womens\"')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#words_2d=unique_words\n",
        "words_2d=split_category_list\n",
        "words_2d = [[w.replace(\"'\", \"\").replace('\"', '').replace('(', '').replace(')', '').replace('[', '').replace(']', '').replace('...', '').replace(',', '') for w in sublist] for sublist in words_2d]\n",
        "\n",
        "# splited into individual words\n",
        "words_2d = [[word for phrase in sublist for word in phrase.split()] for sublist in words_2d]\n",
        "\n",
        "# print(words_2d[1:30])\n",
        "\n",
        "## TF (TERM FREQUENCY) STARTS\n",
        "\n",
        "\n",
        "# print('Term frequency dataframe\\n',tf_df)\n",
        "\n",
        "# empty 2d list bana rahy\n",
        "tf_list = []\n",
        "\n",
        "# Populate the list with empty sublists\n",
        "for i in range(20000):\n",
        "    row=[]\n",
        "    word_row=words_2d[i]\n",
        "    for j in range(len(word_row)):\n",
        "        row.append('')\n",
        "    tf_list.append(row)\n",
        "\n",
        "# print(tf_list)\n",
        "\n",
        "\n",
        "# intialize kardo sub values ko 0\n",
        "for i in range(20000):\n",
        "    word_row=words_2d[i]\n",
        "    for j in range(len(word_row)):\n",
        "        tf_list[i][j]=0\n",
        "\n",
        "# print(tf_list)\n",
        "\n",
        "# words removed doesnot matter as tey will become 0\n",
        "\n",
        "# agar ohi word dubara aey tu +1 kardo kitni bar aik word aya hy aik row my\n",
        "for i in range(20000):\n",
        "    word_row=words_2d[i]\n",
        "    len1=len(word_row)\n",
        "    for j in range(len(word_row)):\n",
        "        for k in range(len(word_row)):\n",
        "            if words_2d[i][j]==words_2d[i][k]:\n",
        "                tf_list[i][j]=tf_list[i][j]+1\n",
        "\n",
        "\n",
        "for i in range(9):\n",
        "  word_row=words_2d[i]\n",
        "  for j in range(len(word_row)): #10\n",
        "    print(words_2d[i][j],end=' ')\n",
        "  print('')\n",
        "\n",
        "for i in range(9):\n",
        "  word_row=words_2d[i]\n",
        "  for j in range(len(word_row)): #10\n",
        "    print(tf_list[i][j],end=' ')\n",
        "  print('')\n",
        "\n",
        "\n",
        "\n",
        "# word ko apni row ki length sy divide kardia\n",
        "for i in range(20000):\n",
        "    word_row=words_2d[i]\n",
        "    len1=len(word_row)\n",
        "    for j in range(len(word_row)):\n",
        "        tf_list[i][j]=round(tf_list[i][j]/len1,2)\n",
        "\n",
        "# print(tf_list[0:30])\n",
        "# print(word_row[0:30])\n",
        "print(len(word_row))\n",
        "for i in range(9):\n",
        "  word_row=words_2d[i]\n",
        "  for j in range(len(word_row)): #10\n",
        "    print(tf_list[i][j],end=' ')\n",
        "  print('')\n",
        "\n",
        "\n",
        "# tf_df = pd.DataFrame(0,index=range(20000),columns=unique_words)\n",
        "# if any(\"Womens\" in col_name for col_name in tf_df.columns):\n",
        "#     print('Yes')\n",
        "# else:\n",
        "#     print('No')\n",
        "\n",
        "\n",
        "# for i in range(20000):\n",
        "#   word_row=words_2d[i]\n",
        "#   for j in range(len(word_row)):\n",
        "#       tf_df.loc[i,words_2d[i][j]]=tf_list[i][j]\n",
        "\n",
        "# tf_df.fillna(0, inplace=True)\n",
        "\n",
        "# tf_df.to_csv('tf_df.csv')\n",
        "# print(tf_df)\n",
        "\n",
        "## TF (TERM FREQUENCY) ENDS\n",
        "\n",
        "# IDF\n",
        "no_of_sentences=len(split_category_list) # no of rows in dataframe(should it not be no of columns)\n",
        "print('No of sentences: ',no_of_sentences)\n",
        "\n",
        "# TF-IDF ends\n",
        "\n",
        "\n",
        "\n",
        "## NAIVE BAYES STARTS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# creating a matrix which contains ratings\n",
        "valid_ratings = df['overall_rating'] != 'No rating available'\n",
        "valid_indices = df.index[valid_ratings]\n",
        "rating = pd.DataFrame({'overall_rating': valid_indices})\n",
        "# print(rating)\n",
        "\n",
        "# creating a new column to put inside the dataframe\n",
        "index=[]\n",
        "for i in range(1,200001):\n",
        "    index.append(f\"index_{i}\")\n",
        "index_no=pd.DataFrame({'index':index})\n",
        "\n",
        "df['index']=index_no\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# how many duplicated items exist\n",
        "pn=df[['product_name']]\n",
        "pn=pn['product_name']\n",
        "dup_pn=sum(pn.duplicated())\n",
        "#print('No of duplicates: ',dup_pn)\n",
        "if pn.duplicated().any():\n",
        "    print('')\n",
        "    # print(pn[pn.duplicated()])\n",
        "\n",
        "# The groupby() method groups the dataframe by the product_name\n",
        "# column and the ngroup() method returns a unique integer for each group\n",
        "\n",
        "\n",
        "df['item_id'] = df.groupby('product_name').ngroup() + 1\n",
        "\n",
        "# search at item_id equals 1\n",
        "filtered_df = df[df['item_id'] == 1][['index','product_name', 'item_id']]\n",
        "# print(filtered_df)\n",
        "\n",
        "# item dataset\n",
        "\n",
        "item_df = df.sort_values(by=['item_id'], ascending=True)\n",
        "item_df['email'] = 'item' + item_df['item_id'].astype(str) + '@gmail.com'\n",
        "item_df['password'] = 'item' + item_df['item_id'].astype(str)\n",
        "item_df = item_df.drop(columns=['product_url', 'uniq_id', 'crawl_timestamp','pid','overall_rating','product_specifications','is_FK_Advantage_product','product_category_tree'])\n",
        "# print(item_df)\n",
        "# item_df.to_csv('sorted_items.csv', index=False)\n",
        "\n",
        "# user dataset\n",
        "\n",
        "user_id1 = np.random.randint(low=1, high=4000, size=20000)\n",
        "user_df = df.copy()\n",
        "user_df['user_id'] = user_id1\n",
        "user_df['email'] = 'user' + user_df['user_id'].astype(str) + '@gmail.com'\n",
        "user_df['password'] = 'user' + user_df['user_id'].astype(str)\n",
        "user_df = user_df.drop(columns=['item_id', 'product_url', 'uniq_id', 'crawl_timestamp','pid','overall_rating','product_specifications','is_FK_Advantage_product','product_category_tree'])\n",
        "user_df = user_df.sort_values(by=['user_id'], ascending=True)\n",
        "# user_df.to_csv('sorted_users.csv', index=False)\n",
        "\n",
        "## NAIVE BAYES ENDS\n"
      ],
      "metadata": {
        "id": "aBM5YdyCQdS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0babc5-f071-4348-bafb-98ee2302462a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clothing Womens Clothing Lingerie Sleep & Swimwear Shorts Alisha Shorts Alisha Solid Womens Cycling Shorts \n",
            "Furniture Living Room Furniture Sofa Beds & Futons FabHomeDecor Fabric Double Sofa Bed Finish Colo \n",
            "Footwear Womens Footwear Ballerinas AW Bellies \n",
            "Clothing Womens Clothing Lingerie Sleep & Swimwear Shorts Alisha Shorts Alisha Solid Womens Cycling Shorts \n",
            "Pet Supplies Grooming Skin & Coat Care Shampoo Sicons All Purpose Arnica Dog Shampoo 500 ml \n",
            "Eternal Gandhi Super Series Crystal Paper Weight \n",
            "Clothing Womens Clothing Lingerie Sleep & Swimwear Shorts Alisha Shorts Alisha Solid Womens Cycling Shorts \n",
            "Furniture Living Room Furniture Sofa Beds & Futons FabHomeDecor Fabric Double Sofa Bed Finish Colo \n",
            "Footwear Womens Footwear Ballerinas dilli bazaaar Bellies Corporate Casuals Casuals \n",
            "2 2 2 1 1 1 1 3 2 3 2 1 2 1 3 \n",
            "2 1 1 2 2 1 1 1 1 1 1 2 1 1 1 \n",
            "2 1 2 1 1 1 \n",
            "2 2 2 1 1 1 1 3 2 3 2 1 2 1 3 \n",
            "1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 \n",
            "1 1 1 1 1 1 1 \n",
            "2 2 2 1 1 1 1 3 2 3 2 1 2 1 3 \n",
            "2 1 1 2 2 1 1 1 1 1 1 2 1 1 1 \n",
            "2 1 2 1 1 1 1 1 2 2 \n",
            "10\n",
            "0.13 0.13 0.13 0.07 0.07 0.07 0.07 0.2 0.13 0.2 0.13 0.07 0.13 0.07 0.2 \n",
            "0.13 0.07 0.07 0.13 0.13 0.07 0.07 0.07 0.07 0.07 0.07 0.13 0.07 0.07 0.07 \n",
            "0.33 0.17 0.33 0.17 0.17 0.17 \n",
            "0.13 0.13 0.13 0.07 0.07 0.07 0.07 0.2 0.13 0.2 0.13 0.07 0.13 0.07 0.2 \n",
            "0.06 0.06 0.06 0.06 0.06 0.06 0.06 0.12 0.06 0.06 0.06 0.06 0.06 0.12 0.06 0.06 \n",
            "0.14 0.14 0.14 0.14 0.14 0.14 0.14 \n",
            "0.13 0.13 0.13 0.07 0.07 0.07 0.07 0.2 0.13 0.2 0.13 0.07 0.13 0.07 0.2 \n",
            "0.13 0.07 0.07 0.13 0.13 0.07 0.07 0.07 0.07 0.07 0.07 0.13 0.07 0.07 0.07 \n",
            "0.2 0.1 0.2 0.1 0.1 0.1 0.1 0.1 0.2 0.2 \n",
            "       A-line  A-maze  A02  A075  A091  A093  A114  A46  A553SA-XX05  A76  \\\n",
            "0           0       0    0     0     0     0     0    0            0    0   \n",
            "1           0       0    0     0     0     0     0    0            0    0   \n",
            "2           0       0    0     0     0     0     0    0            0    0   \n",
            "3           0       0    0     0     0     0     0    0            0    0   \n",
            "4           0       0    0     0     0     0     0    0            0    0   \n",
            "...       ...     ...  ...   ...   ...   ...   ...  ...          ...  ...   \n",
            "19995       0       0    0     0     0     0     0    0            0    0   \n",
            "19996       0       0    0     0     0     0     0    0            0    0   \n",
            "19997       0       0    0     0     0     0     0    0            0    0   \n",
            "19998       0       0    0     0     0     0     0    0            0    0   \n",
            "19999       0       0    0     0     0     0     0    0            0    0   \n",
            "\n",
            "       ...  wt278  www.thepaper.asia  xaiomiredmi1s  xpert  yellow  \\\n",
            "0      ...      0                  0              0      0       0   \n",
            "1      ...      0                  0              0      0       0   \n",
            "2      ...      0                  0              0      0       0   \n",
            "3      ...      0                  0              0      0       0   \n",
            "4      ...      0                  0              0      0       0   \n",
            "...    ...    ...                ...            ...    ...     ...   \n",
            "19995  ...      0                  0              0      0       0   \n",
            "19996  ...      0                  0              0      0       0   \n",
            "19997  ...      0                  0              0      0       0   \n",
            "19998  ...      0                  0              0      0       0   \n",
            "19999  ...      0                  0              0      0       0   \n",
            "\n",
            "       youniqueshop  your  zaidis  zasmina  zizu  \n",
            "0                 0     0       0        0     0  \n",
            "1                 0     0       0        0     0  \n",
            "2                 0     0       0        0     0  \n",
            "3                 0     0       0        0     0  \n",
            "4                 0     0       0        0     0  \n",
            "...             ...   ...     ...      ...   ...  \n",
            "19995             0     0       0        0     0  \n",
            "19996             0     0       0        0     0  \n",
            "19997             0     0       0        0     0  \n",
            "19998             0     0       0        0     0  \n",
            "19999             0     0       0        0     0  \n",
            "\n",
            "[20000 rows x 8089 columns]\n",
            "No of sentences:  20000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_df=pd.read_csv('tf_df.csv')\n",
        "for i in range(10):\n",
        "  word_row=words_2d[i]\n",
        "  for j in range(len(word_row)):\n",
        "      tf_df.loc[i,words_2d[i][j]]=tf_list[i][j]\n",
        "\n",
        "tf_df.fillna(0, inplace=True)\n",
        "print(tf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjJWCeZZklR4",
        "outputId": "6844a6de-d32b-4e68-b439-ab1fc5818498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Unnamed: 0  A-line  A-maze  A02  A075  A091  A093  A114  A46  \\\n",
            "0               0       0       0    0     0     0     0     0    0   \n",
            "1               1       0       0    0     0     0     0     0    0   \n",
            "2               2       0       0    0     0     0     0     0    0   \n",
            "3               3       0       0    0     0     0     0     0    0   \n",
            "4               4       0       0    0     0     0     0     0    0   \n",
            "...           ...     ...     ...  ...   ...   ...   ...   ...  ...   \n",
            "19995       19995       0       0    0     0     0     0     0    0   \n",
            "19996       19996       0       0    0     0     0     0     0    0   \n",
            "19997       19997       0       0    0     0     0     0     0    0   \n",
            "19998       19998       0       0    0     0     0     0     0    0   \n",
            "19999       19999       0       0    0     0     0     0     0    0   \n",
            "\n",
            "       A553SA-XX05  ...  yellow  youniqueshop  your  zaidis  zasmina  zizu  \\\n",
            "0                0  ...       0             0     0       0        0     0   \n",
            "1                0  ...       0             0     0       0        0     0   \n",
            "2                0  ...       0             0     0       0        0     0   \n",
            "3                0  ...       0             0     0       0        0     0   \n",
            "4                0  ...       0             0     0       0        0     0   \n",
            "...            ...  ...     ...           ...   ...     ...      ...   ...   \n",
            "19995            0  ...       0             0     0       0        0     0   \n",
            "19996            0  ...       0             0     0       0        0     0   \n",
            "19997            0  ...       0             0     0       0        0     0   \n",
            "19998            0  ...       0             0     0       0        0     0   \n",
            "19999            0  ...       0             0     0       0        0     0   \n",
            "\n",
            "          &    AW   500    ml  \n",
            "0      0.07  0.00  0.00  0.00  \n",
            "1      0.07  0.00  0.00  0.00  \n",
            "2      0.00  0.17  0.00  0.00  \n",
            "3      0.07  0.00  0.00  0.00  \n",
            "4      0.06  0.00  0.06  0.06  \n",
            "...     ...   ...   ...   ...  \n",
            "19995  0.00  0.00  0.00  0.00  \n",
            "19996  0.00  0.00  0.00  0.00  \n",
            "19997  0.00  0.00  0.00  0.00  \n",
            "19998  0.00  0.00  0.00  0.00  \n",
            "19999  0.00  0.00  0.00  0.00  \n",
            "\n",
            "[20000 rows x 8094 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RUDPNFo2no_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_df.to_csv('tf_df1.csv')"
      ],
      "metadata": {
        "id": "R1N_UCwzmbjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# create an empty matrix of the same size as tf_df\n",
        "tf_array = np.zeros((len(words_2d), len(unique_words)))\n",
        "\n",
        "# update tf_array with the values from tf_list\n",
        "for i, word_row in enumerate(words_2d):\n",
        "    j_indices = [unique_words.index(word) for word in word_row if word in unique_words]\n",
        "    tf_array[i, j_indices] = tf_list[i][:len(j_indices)]\n",
        "\n",
        "# convert tf_array back to a DataFrame\n",
        "tf_df = pd.DataFrame(tf_array, columns=unique_words)\n",
        "\n",
        "# fill NaN values with 0\n",
        "tf_df.fillna(0, inplace=True)\n",
        "\n",
        "print(tf_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTOwSMuEl_BN",
        "outputId": "b821207f-308d-478e-850d-3f470048fb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       A-line  A-maze  A02  A075  A091  A093  A114  A46  A553SA-XX05  A76  \\\n",
            "0         0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "1         0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "2         0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "3         0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "4         0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "...       ...     ...  ...   ...   ...   ...   ...  ...          ...  ...   \n",
            "19995     0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "19996     0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "19997     0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "19998     0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "19999     0.0     0.0  0.0   0.0   0.0   0.0   0.0  0.0          0.0  0.0   \n",
            "\n",
            "       ...  wt278  www.thepaper.asia  xaiomiredmi1s  xpert  yellow  \\\n",
            "0      ...    0.0                0.0            0.0    0.0     0.0   \n",
            "1      ...    0.0                0.0            0.0    0.0     0.0   \n",
            "2      ...    0.0                0.0            0.0    0.0     0.0   \n",
            "3      ...    0.0                0.0            0.0    0.0     0.0   \n",
            "4      ...    0.0                0.0            0.0    0.0     0.0   \n",
            "...    ...    ...                ...            ...    ...     ...   \n",
            "19995  ...    0.0                0.0            0.0    0.0     0.0   \n",
            "19996  ...    0.0                0.0            0.0    0.0     0.0   \n",
            "19997  ...    0.0                0.0            0.0    0.0     0.0   \n",
            "19998  ...    0.0                0.0            0.0    0.0     0.0   \n",
            "19999  ...    0.0                0.0            0.0    0.0     0.0   \n",
            "\n",
            "       youniqueshop  your  zaidis  zasmina  zizu  \n",
            "0               0.0   0.0     0.0      0.0   0.0  \n",
            "1               0.0   0.0     0.0      0.0   0.0  \n",
            "2               0.0   0.0     0.0      0.0   0.0  \n",
            "3               0.0   0.0     0.0      0.0   0.0  \n",
            "4               0.0   0.0     0.0      0.0   0.0  \n",
            "...             ...   ...     ...      ...   ...  \n",
            "19995           0.0   0.0     0.0      0.0   0.0  \n",
            "19996           0.0   0.0     0.0      0.0   0.0  \n",
            "19997           0.0   0.0     0.0      0.0   0.0  \n",
            "19998           0.0   0.0     0.0      0.0   0.0  \n",
            "19999           0.0   0.0     0.0      0.0   0.0  \n",
            "\n",
            "[20000 rows x 8089 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from flask import Flask, render_template, redirect, request, session, url_for, abort\n",
        "#from database import Database\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#app = Flask(__name__)\n",
        "\n",
        "#app.secret_key = 'your_secret_key'\n",
        "\n",
        "# panda ghass khany kyliay nahi balky excell sheet read karny kyliay\n",
        "# or osko dataframe save karny kyliay\n",
        "df=pd.read_csv('sample.csv')\n",
        "\n",
        "#db=Database()\n",
        "\n",
        "\n",
        "index=[]\n",
        "for i in range(20000):\n",
        "    index.append(i)\n",
        "index_no=pd.DataFrame({'index':index})\n",
        "\n",
        "\n",
        "arr_straight = np.arange(4000)\n",
        "\n",
        "# Create array with 16000 random values between 0 and 4000\n",
        "arr_random = np.random.randint(low=0, high=4000, size=16000)\n",
        "\n",
        "# Concatenate the two arrays\n",
        "arr_combined = np.concatenate((arr_straight, arr_random))\n",
        "\n",
        "buyer_id1 = arr_combined\n",
        "\n",
        "\n",
        "# Complete table\n",
        "all_df=df.copy()\n",
        "\n",
        "all_df['index']=index_no\n",
        "\n",
        "for i in range(20000):\n",
        "    if pd.isna(all_df.iloc[i]['brand']) or len(all_df.iloc[i]['brand']) == 0:\n",
        "        all_df.iloc[i, all_df.columns.get_loc('brand')] = 'NewBrand'\n",
        "\n",
        "\n",
        "all_df['b_id'] = buyer_id1\n",
        "\n",
        "all_df['i_id'] = all_df.groupby('product_name').ngroup() + 1\n",
        "all_df['s_id'] = all_df.groupby('brand').ngroup() + 1\n",
        "\n",
        "all_df = all_df.sort_values(by=['index'], ascending=True)\n",
        "\n",
        "\n",
        "all_df['s_name'] = all_df['brand']\n",
        "all_df['s_email'] = 'seller' + all_df['s_id'].astype(str) + '@gmail.com'\n",
        "all_df['s_password'] = 'seller' + all_df['s_id'].astype(str)\n",
        "\n",
        "all_df['b_name'] = 'buyer' + all_df['b_id'].astype(str)\n",
        "all_df['b_email'] = 'buyer' + all_df['b_id'].astype(str) + '@gmail.com'\n",
        "all_df['b_password'] = 'buyer' + all_df['b_id'].astype(str)\n",
        "\n",
        "all_df = all_df.drop(columns=['product_url', 'uniq_id', 'crawl_timestamp','pid','overall_rating','product_specifications','is_FK_Advantage_product','product_category_tree'])\n",
        "# print(len(all_df))\n",
        "#print(all_df)\n",
        "\n",
        "# print(all_df['brand'])\n",
        "\n",
        "brand_size = df['brand'].str.len()\n",
        "\n",
        "all_df.iloc[99, all_df.columns.get_loc('brand')] = 'NewBrand'\n",
        "\n",
        "# print(brand_size[99])\n",
        "\n",
        "#all_df.to_csv('all_df.csv', index=False)\n",
        "\n",
        "\n",
        "# buyer df\n",
        "\n",
        "buyer_df = all_df[['b_id', 'b_name', 'b_email', 'b_password']].copy()\n",
        "buyer_df = buyer_df.sort_values(by=['b_id'], ascending=True)\n",
        "buyer_df = buyer_df.drop_duplicates(subset=['b_id'])\n",
        "#\n",
        "\n",
        "# db.CreateTableBuyer()\n",
        "# db.CreateTableSeller()\n",
        "\n",
        "#\n",
        "# for i in range(len(buyer_df)):\n",
        "#     b_id = buyer_df.iloc[i, 0]\n",
        "#     b_name = buyer_df.iloc[i,1]\n",
        "#     b_email = buyer_df.iloc[i,2]\n",
        "#     b_password = buyer_df.iloc[i,3]\n",
        "#\n",
        "#     print('bid:',b_id)\n",
        "#     # print('b_name: ',b_name)\n",
        "#\n",
        "#     db.insertIntoBuyer(b_id, b_name, b_email, b_password)\n",
        "#\n",
        "\n",
        "\n",
        "# items=db.returnBuyer()\n",
        "# print(items)\n",
        "#buyer_df.to_csv('buyer_df.csv',index=False)\n",
        "\n",
        "# seller df\n",
        "\n",
        "seller_df = all_df[['s_id','s_name', 's_email', 's_password']].copy()\n",
        "seller_df = seller_df.sort_values(by=['s_id'], ascending=True)\n",
        "seller_df = seller_df.drop_duplicates(subset=['s_id'])\n",
        "#seller_df.to_csv('seller_df.csv',index=False)\n",
        "\n",
        "# for i in range(len(seller_df)):\n",
        "#\n",
        "#     s_id = seller_df.iloc[i, 0]\n",
        "#     s_name = seller_df.iloc[i, 1]\n",
        "#     s_email = seller_df.iloc[i, 2]\n",
        "#     s_password = seller_df.iloc[i, 3]\n",
        "#\n",
        "#     print('sid:',s_id)\n",
        "#     print('s_name: ',s_name)\n",
        "#\n",
        "#     db.insertIntoSeller(s_id, s_name, s_email, s_password)\n",
        "\n",
        "# items=db.returnSeller()\n",
        "# print(items)\n",
        "\n",
        "# item df\n",
        "item_df = all_df.drop(columns=['b_name', 'b_email', 'b_password', 's_name', 's_email', 's_password'])\n",
        "item_df['image'] = item_df['image'].str.replace('[', '').str.replace(']', '').str.replace('\"', '')\n",
        "item_df['image'] = item_df['image'].str.split(',').str[0]\n",
        "#item_df.to_csv('item_df.csv', index=False)\n",
        "#\n",
        "# db.CreateTableItem()\n",
        "\n",
        "#product_name\tretail_price\tdiscounted_price\timage\tdescription\tproduct_rating\tbrand\tindex\tb_id\ti_id\ts_id\n",
        "# for i in range(len(item_df)):\n",
        "#     product_name = item_df.iloc[i, 0]\n",
        "#     retail_price = item_df.iloc[i, 1]\n",
        "#     discounted_price = item_df.iloc[i, 2]\n",
        "#     image = item_df.iloc[i, 3]\n",
        "#     description = item_df.iloc[i, 4]\n",
        "#     product_rating = item_df.iloc[i, 5]\n",
        "#     brand = item_df.iloc[i, 6]\n",
        "#     item_index = item_df.iloc[i, 7]\n",
        "#     buyer_id = item_df.iloc[i, 8] + 1\n",
        "#     i_id = item_df.iloc[i, 9]\n",
        "#     seller_id = item_df.iloc[i, 10]\n",
        "#     print('buyer_id:', buyer_id)\n",
        "#     print('retail_price: ', retail_price)\n",
        "#     db.insertIntoItem(item_index, product_name, retail_price, discounted_price, image, description, product_rating, brand, buyer_id, i_id, seller_id)\n",
        "\n",
        "# items = db.returnItem()\n",
        "# print(items)\n",
        "\n"
      ],
      "metadata": {
        "id": "WEkfspV4eaCY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}